{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7d2398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from datasets) (0.35.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: anyio in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'citation-function' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\#ANONYMIZED\\downloads\\synthdata1\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers\n",
    "!pip install -U datasets\n",
    "!git clone https://github.com/davidjurgens/citation-function\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf1de439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open('api_openrouter.txt', 'r') as f:\n",
    "  API=f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a75ca2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\#ANONYMIZED\\Downloads\\synthdata1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"hrithikpiyush/acl-arc\")\n",
    "ds.set_format(\"pandas\")\n",
    "train=ds[\"train\"][:]\n",
    "validation=ds[\"validation\"][:]\n",
    "test=ds[\"test\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812854e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 867\n",
      "1 317\n",
      "2 305\n",
      "3 63\n",
      "4 76\n",
      "5 60\n"
     ]
    }
   ],
   "source": [
    "for df in train.groupby('intent'):\n",
    "    print(df[0],len(df[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "880ebea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste=[]\n",
    "for texte in train['cleaned_cite_text']:\n",
    "  liste.append(texte.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc61eaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you are a Postdoctoral Researcher specializing in Econometrics. Your task is to rewrite the following citation (@@CITATION) whose goal is to provide relevant information for this domain (BACKGROUND). 10 times in different ways, the said citations (@@CITATION) should keep the same citation intent but change the scientific domain. Each citation should be different but keep the same spirit.\n",
      "Imagine you are a Researcher in Industry specializing in Environmental Geography. Your task is to rewrite the following citation (@@CITATION) whose goal is  to use data, methods, etc from the citation. (USES)  10 times in different ways, the said citations (@@CITATION) should keep the same citation intent but change the scientific domain. Each citation should be different but keep the same spirit.\n",
      "Imagine you are a Postdoctoral Researcher specializing in Urban Geography. Your task is to rewrite the following citation (@@CITATION) whose goal is to express similarity/differences to the citation (COMPARES OR CONTRASTS) 10 times in different ways, the said citations (@@CITATION) should keep the same citation intent but change the scientific domain. Each citation should be different but keep the same spirit.\n",
      "Imagine you are a Postdoctoral Researcher specializing in Environmental Policy and Management. Your task is to rewrite the following citation (@@CITATION) whose goal is to illustrate need for data, goals, methods, etc.(MOTIVATION) 10 times in different ways, the said citations (@@CITATION) should keep the same citation intent but change the scientific domain. Each citation should be different but keep the same spirit.\n",
      "Imagine you are a Full Professor specializing in Cosmology. Your task is to rewrite the following citation (@@CITATION) whose goal is to extend the citation's data, methods, etc. (CONTINUATION) 10 times in different ways, the said citations (@@CITATION) should keep the same citation intent but change the scientific domain. Each citation should be different but keep the same spirit.\n",
      "Imagine you are a Associate Professor specializing in Planetary Science. Your task is to rewrite the following citation (@@CITATION) whose goal is to be a potential avenue for future work (FUTURE) 10 times in different ways, the said citations (@@CITATION) should keep the same citation intent but change the scientific domain. Each citation should be different but keep the same spirit.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "ds = load_dataset(\"hrithikpiyush/acl-arc\")\n",
    "ds.set_format(\"pandas\")\n",
    "train=ds[\"train\"][:]\n",
    "validation=ds[\"validation\"][:]\n",
    "test=ds[\"test\"][:]\n",
    "academic_personas = [\n",
    "    \"PhD Student\",\n",
    "    \"Postdoctoral Researcher\",\n",
    "    \"Assistant Professor\",\n",
    "    \"Associate Professor\",\n",
    "    \"Full Professor\",\n",
    "    \"Researcher in Industry\",\n",
    "]\n",
    "\n",
    "disciplines = [\n",
    "        \"Artificial Intelligence\",\n",
    "        \"Machine Learning\",\n",
    "        \"Computer Vision\",\n",
    "        \"Natural Language Processing\",\n",
    "        \"Cybersecurity\",\n",
    "        \"Distributed Systems\",\n",
    "        \"Algorithms and Complexity\",\n",
    "        \"Databases\",\n",
    "        \"Human-Computer Interaction\",\n",
    "        \"Quantum Computing\",\n",
    "        \"Embedded Systems\",\n",
    "        \"Software Engineering\",\n",
    "        \"Classical Mechanics\",\n",
    "        \"Quantum Mechanics\",\n",
    "        \"Relativity\",\n",
    "        \"Condensed Matter Physics\",\n",
    "        \"Particle Physics\",\n",
    "        \"Nuclear Physics\",\n",
    "        \"Optics and Photonics\",\n",
    "        \"Statistical Mechanics\",\n",
    "        \"Astrophysics\",\n",
    "        \"Acoustics\",\n",
    "        \"Plasma Physics\",\n",
    "        \"Biophysics\",\n",
    "        \"Algebra\",\n",
    "        \"Real Analysis\",\n",
    "        \"Complex Analysis\",\n",
    "        \"Topology\",\n",
    "        \"Differential Geometry\",\n",
    "        \"Number Theory\",\n",
    "        \"Probability Theory\",\n",
    "        \"Statistics\",\n",
    "        \"Numerical Analysis\",\n",
    "        \"Optimization\",\n",
    "        \"Mathematical Logic\",\n",
    "        \"Applied Mathematics\",\n",
    "        \"Organic Chemistry\",\n",
    "        \"Inorganic Chemistry\",\n",
    "        \"Physical Chemistry\",\n",
    "        \"Analytical Chemistry\",\n",
    "        \"Biochemistry\",\n",
    "        \"Materials Chemistry\",\n",
    "        \"Theoretical and Computational Chemistry\",\n",
    "        \"Environmental Chemistry\",\n",
    "        \"Polymer Chemistry\",\n",
    "        \"Surface Chemistry\",\n",
    "        \"Molecular Biology\",\n",
    "        \"Cell Biology\",\n",
    "        \"Genetics and Genomics\",\n",
    "        \"Microbiology\",\n",
    "        \"Neurobiology\",\n",
    "        \"Ecology\",\n",
    "        \"Evolutionary Biology\",\n",
    "        \"Developmental Biology\",\n",
    "        \"Physiology\",\n",
    "        \"Botany\",\n",
    "        \"Zoology\",\n",
    "        \"Systems Biology\",\n",
    "        \"Internal Medicine\",\n",
    "        \"Surgery\",\n",
    "        \"Pediatrics\",\n",
    "        \"Psychiatry\",\n",
    "        \"Cardiology\",\n",
    "        \"Oncology\",\n",
    "        \"Neurology\",\n",
    "        \"Radiology\",\n",
    "        \"Anesthesiology\",\n",
    "        \"Emergency Medicine\",\n",
    "        \"Primary Care\",\n",
    "        \"Public Health and Epidemiology\",\n",
    "        \"Mechanical Engineering\",\n",
    "        \"Electrical Engineering\",\n",
    "        \"Civil Engineering\",\n",
    "        \"Chemical Engineering\",\n",
    "        \"Aerospace Engineering\",\n",
    "        \"Biomedical Engineering\",\n",
    "        \"Environmental Engineering\",\n",
    "        \"Materials Engineering\",\n",
    "        \"Industrial Engineering\",\n",
    "        \"Computer Engineering\",\n",
    "        \"Robotics\",\n",
    "        \"Systems Engineering\",\n",
    "        \"Microeconomics\",\n",
    "        \"Macroeconomics\",\n",
    "        \"Econometrics\",\n",
    "        \"Behavioral Economics\",\n",
    "        \"Development Economics\",\n",
    "        \"International Economics\",\n",
    "        \"Public Economics\",\n",
    "        \"Environmental Economics\",\n",
    "        \"Health Economics\",\n",
    "        \"Labor Economics\",\n",
    "        \"Financial Economics\",\n",
    "        \"Cognitive Psychology\",\n",
    "        \"Developmental Psychology\",\n",
    "        \"Social Psychology\",\n",
    "        \"Clinical Psychology\",\n",
    "        \"Neuropsychology\",\n",
    "        \"Industrial-Organizational Psychology\",\n",
    "        \"Health Psychology\",\n",
    "        \"Personality Psychology\",\n",
    "        \"Psychometrics\",\n",
    "        \"Educational Psychology\",\n",
    "        \"Sociological Theory\",\n",
    "        \"Social Stratification\",\n",
    "        \"Cultural Sociology\",\n",
    "        \"Sociology of Education\",\n",
    "        \"Urban Sociology\",\n",
    "        \"Medical Sociology\",\n",
    "        \"Economic Sociology\",\n",
    "        \"Political Sociology\",\n",
    "        \"Family Sociology\",\n",
    "        \"Quantitative Methods\",\n",
    "        \"Qualitative Methods\",\n",
    "        \"Mineralogy and Petrology\",\n",
    "        \"Structural Geology\",\n",
    "        \"Sedimentology\",\n",
    "        \"Stratigraphy\",\n",
    "        \"Paleontology\",\n",
    "        \"Geomorphology\",\n",
    "        \"Geochemistry\",\n",
    "        \"Geophysics\",\n",
    "        \"Hydrogeology\",\n",
    "        \"Environmental Geology\",\n",
    "        \"Remote Sensing in Geology\",\n",
    "        \"Observational Astronomy\",\n",
    "        \"Theoretical Astronomy\",\n",
    "        \"Planetary Science\",\n",
    "        \"Stellar Astrophysics\",\n",
    "        \"Extragalactic Astronomy\",\n",
    "        \"Cosmology\",\n",
    "        \"Astrochemistry\",\n",
    "        \"High-energy Astrophysics\",\n",
    "        \"Radio Astronomy\",\n",
    "        \"Infrared and Optical Astronomy\",\n",
    "        \"Climate Science\",\n",
    "        \"Conservation Biology\",\n",
    "        \"Environmental Chemistry\",\n",
    "        \"Ecology\",\n",
    "        \"Environmental Policy and Management\",\n",
    "        \"Hydrology\",\n",
    "        \"Soil Science\",\n",
    "        \"Atmospheric Science\",\n",
    "        \"Sustainability Science\",\n",
    "        \"Environmental Impact Assessment\",\n",
    "        \"Comparative Politics\",\n",
    "        \"International Relations\",\n",
    "        \"Political Theory\",\n",
    "        \"Public Policy\",\n",
    "        \"Political Economy\",\n",
    "        \"Electoral Studies\",\n",
    "        \"Political Behavior\",\n",
    "        \"Public Administration\",\n",
    "        \"Security Studies\",\n",
    "        \"Governance and Institutions\",\n",
    "        \"Ancient History\",\n",
    "        \"Medieval History\",\n",
    "        \"Early Modern History\",\n",
    "        \"Modern History\",\n",
    "        \"Economic History\",\n",
    "        \"Social and Cultural History\",\n",
    "        \"Military History\",\n",
    "        \"History of Science and Technology\",\n",
    "        \"Oral History\",\n",
    "        \"Public History\",\n",
    "        \"Phonetics\",\n",
    "        \"Phonology\",\n",
    "        \"Morphology\",\n",
    "        \"Syntax\",\n",
    "        \"Semantics\",\n",
    "        \"Pragmatics\",\n",
    "        \"Sociolinguistics\",\n",
    "        \"Psycholinguistics\",\n",
    "        \"Computational Linguistics\",\n",
    "        \"Historical Linguistics\",\n",
    "        \"Field Linguistics\",\n",
    "        \"Cultural Anthropology\",\n",
    "        \"Biological Anthropology\",\n",
    "        \"Archaeology\",\n",
    "        \"Linguistic Anthropology\",\n",
    "        \"Medical Anthropology\",\n",
    "        \"Economic Anthropology\",\n",
    "        \"Urban Anthropology\",\n",
    "        \"Ethnography\",\n",
    "        \"Visual Anthropology\",\n",
    "        \"Anthropology of Religion\",\n",
    "        \"Metaphysics\",\n",
    "        \"Epistemology\",\n",
    "        \"Ethics\",\n",
    "        \"Political Philosophy\",\n",
    "        \"Philosophy of Mind\",\n",
    "        \"Philosophy of Science\",\n",
    "        \"Logic\",\n",
    "        \"Aesthetics\",\n",
    "        \"Philosophy of Language\",\n",
    "        \"History of Philosophy\",\n",
    "        \"Physical Geography\",\n",
    "        \"Human Geography\",\n",
    "        \"Geographic Information Systems (GIS)\",\n",
    "        \"Urban Geography\",\n",
    "        \"Economic Geography\",\n",
    "        \"Political Geography\",\n",
    "        \"Cultural Geography\",\n",
    "        \"Environmental Geography\",\n",
    "        \"Cartography\",\n",
    "        \"Remote Sensing\",\n",
    "        \"Ancient Art\",\n",
    "        \"Medieval Art\",\n",
    "        \"Renaissance Art\",\n",
    "        \"Baroque and Rococo\",\n",
    "        \"Modern Art\",\n",
    "        \"Contemporary Art\",\n",
    "        \"Non-Western Art Histories\",\n",
    "        \"Iconography\",\n",
    "        \"Museum Studies\",\n",
    "        \"Conservation and Restoration\",\n",
    "        \"Harmony\",\n",
    "        \"Counterpoint\",\n",
    "        \"Form and Analysis\",\n",
    "        \"Aural Skills\",\n",
    "        \"Tonal Theory\",\n",
    "        \"Atonal and Serial Techniques\",\n",
    "        \"Ethnomusicology\",\n",
    "        \"Music Cognition\",\n",
    "        \"Music Notation and Editorial Practices\",\n",
    "        \"Contemporary Music Theory\",\n",
    "        \"Comparative Literature\",\n",
    "        \"Literary Theory and Criticism\",\n",
    "        \"Medieval and Early Modern Literature\",\n",
    "        \"Modern and Contemporary Literature\",\n",
    "        \"Postcolonial Literature\",\n",
    "        \"Genre Studies\",\n",
    "        \"Narrative Theory\",\n",
    "        \"Digital Humanities and Literature\",\n",
    "        \"Translation Studies\",\n",
    "        \"Children's and Young Adult Literature\",\n",
    "        \"Curriculum and Instruction\",\n",
    "        \"Educational Psychology\",\n",
    "        \"Assessment and Evaluation\",\n",
    "        \"Educational Policy\",\n",
    "        \"Special Education\",\n",
    "        \"Early Childhood Education\",\n",
    "        \"Higher Education Studies\",\n",
    "        \"Technology in Education\",\n",
    "        \"Adult and Continuing Education\",\n",
    "        \"Multicultural Education\"\n",
    "    ]\n",
    "\n",
    "disciplines_1 = [\n",
    "    \"Computer Science\",\n",
    "    \"Physics\",\n",
    "    \"Mathematics\",\n",
    "    \"Chemistry\",\n",
    "    \"Biology\",\n",
    "    \"Medicine\",\n",
    "    \"Engineering\",\n",
    "    \"Economics\",\n",
    "    \"Psychology\",\n",
    "    \"Sociology\",\n",
    "    \"Geology\",\n",
    "    \"Astronomy\",\n",
    "    \"Environmental Science\",\n",
    "    \"Political Science\",\n",
    "    \"History\",\n",
    "    \"Linguistics\",\n",
    "    \"Anthropology\",\n",
    "    \"Philosophy\",\n",
    "    \"Geography\",\n",
    "    \"Art History\",\n",
    "    \"Music Theory\",\n",
    "    \"Literature\",\n",
    "    \"Education\"\n",
    "]\n",
    "\n",
    "place_sentence=['beginning', 'middle', 'end']\n",
    "citation_intent=['BACKGROUND','USES' ,'COMPARES OR CONTRASTS' ,'MOTIVATION', 'CONTINUATION', 'FUTURE' ]\n",
    "prompt_intent={\n",
    "    \"BACKGROUND\":\"to provide relevant information for this domain (BACKGROUND).\",\n",
    "    \"MOTIVATION\":\"to illustrate need for data, goals, methods, etc.(MOTIVATION)\",\n",
    "    \"USES\":\" to use data, methods, etc from the citation. (USES) \",  \n",
    "    \"CONTINUATION\":\"to extend the citation's data, methods, etc. (CONTINUATION)\",\n",
    "    \"COMPARES OR CONTRASTS\":\"to express similarity/differences to the citation (COMPARES OR CONTRASTS)\",\n",
    "    \"FUTURE\": \"to be a potential avenue for future work (FUTURE)\"\n",
    "}\n",
    "\n",
    "citation_dict={\n",
    "    0:\"BACKGROUND\",\n",
    "    1:\"USES\",\n",
    "    2:\"COMPARES OR CONTRASTS\",\n",
    "    3:\"MOTIVATION\",\n",
    "    4:\"CONTINUATION\",\n",
    "    5:\"FUTURE\"\n",
    "}\n",
    "import random\n",
    "\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"hrithikpiyush/acl-arc\")\n",
    "ds.set_format(\"pandas\")\n",
    "train=ds[\"train\"][:]\n",
    "validation=ds[\"validation\"][:]\n",
    "test=ds[\"test\"][:]\n",
    "\n",
    "\n",
    "\n",
    "def generate_researcher_prompt(numero_classe,academic_personas=academic_personas, disciplines=disciplines, citation_intent=citation_intent,prompt_intent=prompt_intent):\n",
    "  persona = random.choice(academic_personas)\n",
    "  discipline = random.choice(disciplines)\n",
    "  intent=citation_dict[numero_classe]\n",
    "  intent=prompt_intent[intent]\n",
    "  prompt = f\"Imagine you are a {persona} specializing in {discipline}. Your task is to rewrite the following citation (@@CITATION) whose goal is {intent} 10 times in different ways, the said citations (@@CITATION) should keep the same citation intent but change the scientific domain. Each citation should be different but keep the same spirit.\"\n",
    "  return prompt\n",
    "\n",
    "# Example usage:\n",
    "print(generate_researcher_prompt(numero_classe=0))\n",
    "print(generate_researcher_prompt(numero_classe=1))\n",
    "print(generate_researcher_prompt(numero_classe=2))\n",
    "print(generate_researcher_prompt(numero_classe=3))\n",
    "print(generate_researcher_prompt(numero_classe=4))\n",
    "print(generate_researcher_prompt(numero_classe=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3679adcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imagine you are a Associate Professor specializing in Cultural Anthropology. Your task is to rewrite the following citation (@@CITATION) whose goal is to provide relevant information for this domain (BACKGROUND). 10 times in different ways, the said citations (@@CITATION) should keep the same citation intent but change the scientific domain. Each citation should be different but keep the same spirit.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_researcher_prompt(numero_classe=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56a30e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt,label in zip(train['cleaned_cite_text'],train['intent']):\n",
    "    PROMPT=(generate_researcher_prompt(label)+' here is the citation:',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94299fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_string(id):\n",
    "  try:\n",
    "    with open('/content/citation-function/data/adjudicated-and-supplemental/'+id+'-parscit.130908.xml.txt', 'r') as f:\n",
    "      file_contents = f.read()\n",
    "  except FileNotFoundError:\n",
    "    with open('/content/citation-function/data/adjudicated-and-supplemental/'+id+'.xml.txt', 'r') as f:\n",
    "      file_contents = f.read()\n",
    "\n",
    "\n",
    "  return file_contents\n",
    "\n",
    "def string_few_shots(nb_of_citations):\n",
    "  output_string = \"\"\n",
    "  for j in range(0,6): #pour chaque classe\n",
    "    compteur=nb_of_citations\n",
    "    for count,letuple in enumerate (zip(train['intent'],train['cleaned_cite_text'],train['section_name'])): #on parcourt toutes les citations\n",
    "      if(letuple[0] == j): #si elle a la bonne classe\n",
    "        #output_string += f\"CLASS: {citation_dict[j]}  SECTION NAME:{letuple[2]} CITATION: {letuple[1]}\\n\" #on l'ajoute à la string\n",
    "        output_string += f\"CLASS: {citation_dict[j]} CITATION: {letuple[1]}\\n\" #on l'ajoute à la string\n",
    "\n",
    "        compteur-=1 # et on décrémente\n",
    "        if(compteur==0): #si on arrive à zéro on passe à la classe d'après\n",
    "          break\n",
    "  return output_string\n",
    "\n",
    "\n",
    "\n",
    "def string_few_shots_2(nb_of_citations,class_number):\n",
    "  output_string = \"\"\n",
    "  j=class_number\n",
    "  compteur=nb_of_citations\n",
    "  for count,letuple in enumerate (zip(train['intent'],train['cleaned_cite_text'],train['section_name'])): #on parcourt toutes les citations\n",
    "    if(letuple[0] == j): #si elle a la bonne classe\n",
    "      #output_string += f\"CLASS: {citation_dict[j]}  SECTION NAME:{letuple[2]} CITATION: {letuple[1]}\\n\" #on l'ajoute à la string\n",
    "      output_string += f\"CLASS: {citation_dict[j]} CITATION: {letuple[1]}\\n\" #on l'ajoute à la string\n",
    "\n",
    "      compteur-=1 # et on décrémente\n",
    "      if(compteur==0): #si on arrive à zéro on passe à la classe d'après\n",
    "        break\n",
    "  return output_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d57c964e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imagine you are a PhD Student specializing in Modern and Contemporary Literature. Your task is to rewrite the following citation (@@CITATION) whose goal is to provide relevant information for this domain (BACKGROUND). 10 times in different ways, the said citations (@@CITATION) should keep the same citation intent but change the scientific domain. Each citation should be different but keep the same spirit.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_researcher_prompt(numero_classe=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "046364ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "4\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "4\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "4\n",
      "5\n",
      "0\n",
      "4\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "4\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "5\n",
      "1\n",
      "2\n",
      "2\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "4\n",
      "2\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "5\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "4\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "4\n",
      "1\n",
      "4\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "5\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "2\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "2\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "5\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "2\n",
      "0\n",
      "4\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "4\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "4\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "5\n",
      "5\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "5\n",
      "0\n",
      "4\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "4\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "2\n",
      "2\n",
      "5\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "5\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "4\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "5\n",
      "2\n",
      "0\n",
      "0\n",
      "5\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "5\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "5\n",
      "4\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "5\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "0\n",
      "4\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "5\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "4\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "4\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "5\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "4\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "5\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "4\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "5\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "4\n",
      "2\n",
      "1\n",
      "5\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "1\n",
      "4\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "5\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "5\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "4\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "4\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "4\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "3\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for txt,label in zip(train['cleaned_cite_text'],train['intent']):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dddad65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_liste=[0,0,0,0,0,0]\n",
    "liste_textes=[]\n",
    "for txt,label in zip(train['cleaned_cite_text'],train['intent']):\n",
    "    if label_liste[label]<10:\n",
    "        label_liste[label]+=1\n",
    "        liste_textes.append((txt,label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86d7aa1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( @@CITATION ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .',\n",
       " 0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_textes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5340e1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'gen-1760901608-Dc6HRbDZG0Zg4HY3Inmx', 'provider': 'Crusoe', 'model': 'deepseek/deepseek-chat-v3-0324', 'object': 'chat.completion', 'created': 1760901608, 'choices': [{'logprobs': None, 'finish_reason': 'stop', 'native_finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': \"Certainly! Below are 10 rewritten citations, each preserving the original intent but adapted to different scientific domains while maintaining a similar structure and academic tone.  \\n\\n---\\n\\n1. **Climate Science**:  \\n   Thus, over the past few years, alongside advances in the application of climate models and remote sensing techniques for global temperature predictions (@@CITATION; Mann et al., 1998; Hansen et al., 2001;IPCC, 2021), significant progress has been made in using machine learning to detect localized climate anomalies—extreme weather events or deviations in seasonal patterns (Trenberth et al., 2014; Reichstein et al., 2019; Diffenbaugh et al., 2005; Hidalgo et al., 2000; Mitchell et al., 2002).  \\n\\n2. **Ecology**:  \\n   Thus, in recent years, along with improvements in biodiversity assessment through species distribution modeling (@@CITATION; Elith et al., 2006; Phillips et al., 2006; Guisan & Zimmermann, 2000), notable advances have been achieved in the use of statistical approaches to identify habitat fragmentation and species interaction networks (Hanski, 1998; Urban et al., 2008; Bascompte & Jordano, 2007; Tilman et al., 1994; McGill et al., 2006).  \\n\\n3. **Hydrology**:  \\n   Thus, over the past decade, in tandem with developments in hydrological modeling for watershed assessments (@@CITATION; Beven & Kirkby, 1979; Sivapalan et al., 2003; Abbott et al., 1986), substantial progress has been made in employing statistical techniques to detect flow regime variations and pollutant dispersion patterns (Leopold & Maddock, 1953; Rodriquez-Iturbe & Rinaldo, 1997; Poff et al., 1997; Khan et al., 2013).  \\n\\n4. **Urban Planning**:  \\n   Thus, in recent years, alongside advancements in GIS-based urban sprawl analysis (@@CITATION; Batty & Longley, 1994; Torrens & Alberti, 2000; Silva et al., 2017), significant strides have been made in applying machine learning to assess transit accessibility and land-use efficiency (Anas et al., 1998; Ewing & Cervero, 2010; Bertaud & Malpezzi, 2003; Chen et al., 2021).  \\n\\n5. **Environmental Policy**:  \\n   Thus, over the past decade, with the refinement of cost-benefit models for sustainability assessments (@@CITATION; Pearce et al., 2006; Stern, 2007; Dasgupta, 2021), there has been notable progress in applying econometric techniques to evaluate policy impacts on emission reductions and conservation efforts (Nordhaus, 2019; Ostrom, 1990; Aldy et al., 2003; Stavins, 2020).  \\n\\n6. **Geomorphology**:  \\n   Thus, in recent years, alongside improvements in digital elevation modeling for terrain evolution studies (@@CITATION; Pelletier, 2008; Dietrich et al., 2003; Montgomery, 2002), considerable advances have been made in utilizing statistical methods to analyze erosion patterns and sediment transport (Kirkby & Statham, 1975; Whipple & Tucker, 1999; Tucker & Hancock, 2010).  \\n\\n7. **Conservation Biology**:  \\n   Thus, over the past few years, in parallel with advances in genetic tools for population viability assessment (@@CITATION; Lande, 1993; Frankham et al., 2002; Allendorf et al., 2012), significant progress has been made in using spatial analytics to identify critical wildlife corridors and migration shifts (Crooks & Sanjayan, 2006; Hilty et al., 2006; Lawler et al., 2013).  \\n\\n8. **Atmospheric Science**:  \\n   Thus, in recent years, concomitant with progress in atmospheric circulation models for climate projections (@@CITATION; Lorenz, 1967; Holton, 2004; Collins et al., 2006), substantial developments have occurred in applying machine learning to detect air pollution hotspots and aerosol interactions (Seinfeld & Pandis, 2006; Zhang et al., 2012; Ma et al., 2019).  \\n\\n9. **Soil Science**:  \\n   Thus, over the past decade, alongside advancements in pedotransfer functions for soil property estimation (@@CITATION; McBratney et al., 2002; Pachepsky & Rawls, 2004), marked progress has been achieved in employing spectral analysis to classify carbon sequestration and erosion susceptibility (Viscarra Rossel et al., 2006; Minasny & McBratney, 2006; Lal, 2004).  \\n\\n10. **Sustainable Development**:  \\n    Thus, in recent years, along with improvements in life-cycle assessment frameworks for sustainable resource use (@@CITATION; Finnveden et al., 2009; Bjørn et al., 2018; Guinée et al., 2002), significant strides have been made in integrating econometric techniques to evaluate trade-offs between development and ecosystem services (Costanza et al., 1997; Turner et al., 2003; Dasgupta & Mäler, 1994).  \\n\\n---\\n\\nEach of these variations maintains the original citation's intent—highlighting advances in a methodological approach within a domain—while adapting the content to different subfields of environmental geography and related disciplines. Let me know if you'd like any refinements!\", 'refusal': None, 'reasoning': None}}], 'usage': {'prompt_tokens': 250, 'completion_tokens': 1185, 'total_tokens': 1435}}\n",
      "1 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     23\u001b[39m   \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[32m     24\u001b[39m   payload = {\n\u001b[32m     25\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdeepseek/deepseek-chat-v3-0324\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m }\n\u001b[32m     44\u001b[39m   }\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m   response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m   les_rez.append(response.json())\n\u001b[32m     47\u001b[39m   dico_rez[label].append(response.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\#ANONYMIZED\\Downloads\\synthdata1\\.venv\\Lib\\site-packages\\requests\\api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\#ANONYMIZED\\Downloads\\synthdata1\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\#ANONYMIZED\\Downloads\\synthdata1\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\#ANONYMIZED\\Downloads\\synthdata1\\.venv\\Lib\\site-packages\\requests\\sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\#ANONYMIZED\\Downloads\\synthdata1\\.venv\\Lib\\site-packages\\requests\\models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\#ANONYMIZED\\Downloads\\synthdata1\\.venv\\Lib\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\#ANONYMIZED\\Downloads\\synthdata1\\.venv\\Lib\\site-packages\\urllib3\\response.py:1088\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\#ANONYMIZED\\Downloads\\synthdata1\\.venv\\Lib\\site-packages\\urllib3\\response.py:1248\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1245\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\#ANONYMIZED\\Downloads\\synthdata1\\.venv\\Lib\\site-packages\\urllib3\\response.py:1167\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1168\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "les_rez=[]\n",
    "dico_rez={0:[],1:[],2:[],3:[],4:[],5:[]}\n",
    "\n",
    "url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "i=0\n",
    "\n",
    "headers = {\n",
    "  \"Authorization\": f\"Bearer {API}\",\n",
    "  \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "for binome in liste_textes:\n",
    "    txt=binome[0]\n",
    "    label=binome[1]\n",
    "    i+=1\n",
    "    for j in range(0,10):\n",
    "      print(i,j)\n",
    "      PROMPT=(generate_researcher_prompt(numero_classe=label)+'\\n here is the CITATION'+txt)  \n",
    "      #try:\n",
    "      payload = {\n",
    "          \"model\": \"deepseek/deepseek-chat-v3-0324\",\n",
    "        \"messages\": [\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": PROMPT\n",
    "          }\n",
    "        ]\n",
    "          ,\n",
    "      #   \"provider\": {\n",
    "      #     \"allow_fallbacks\": True,\n",
    "      #     \"quantizations\": [\n",
    "      #       \"fp8\"\n",
    "      #     ]\n",
    "      # },\n",
    "          \"reasoning\": {\n",
    "\n",
    "        \"enabled\": True  # Use high reasoning effort\n",
    "\n",
    "    }\n",
    "      }\n",
    "      response = requests.post(url, headers=headers, json=payload)\n",
    "      les_rez.append(response.json())\n",
    "      dico_rez[label].append(response.json())\n",
    "    #except:\n",
    "      #  print(\"error\")\n",
    "      #  continue\n",
    "      print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b38f4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    for elt in dico_rez[i]:\n",
    "        with open(f'DATASET8/results_openrouter_intent_{i}.txt', 'a',encoding='utf-8') as f:\n",
    "            f.write(elt['choices'][0]['message']['content'] + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fabde5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
